import torch as tr
from torch import nn as nn
from torch.nn import functional as F
from torch import optim as optim
from torch.utils.data import DataLoader
from time import time
from torch.autograd import Variable
from util import ExecDataset, PAD_IDX
from torch.utils.data.sampler import SubsetRandomSampler


class GatedConvolution(nn.Module):
    def __init__(self):
        super(GatedConvolution, self).__init__()
        self.conv = nn.Conv1d(4, 128, 500, stride=500)
        self.convg = nn.Conv1d(4, 128, 500, stride=500)

    def forward(self, x):
        """

        :param x:
        :type x: tr.Tensor
        :return:
        """
        x.transpose_(-1, -2)
        c = self.conv(x.narrow(-2, 0, 4))
        g = F.sigmoid(self.convg(x.narrow(-2, 4, 4)))
        x = F.relu(g * c)
        return F.max_pool1d(x, x.size()[2:]).view(-1, 128)


def accuracy_on(net, dev_loader, loss_fn):
    total_loss = good = 0.0
    for x, y in dev_loader:
        x, y = tr.tensor(x.long()), tr.tensor(y.long())
        out = net(x)
        loss = loss_fn(out, y)
        total_loss += loss.item()
        if out.detach().numpy().argmax() == y.detach().numpy().max():
            good += out.size()[0]
    return total_loss / len(dev_loader), good / len(dev_loader) * 100.0


def train_on(net, optimizer, loss_fn, train_loader, dev_loader, epochs=5):
    print "epoch", "time", "train avg. loss", "(dev avg.loss, accuracy)"
    for i in xrange(epochs):
        start = time()
        total_loss = 0.0
        for x, y in train_loader:
            optimizer.zero_grad()
            x, y = tr.tensor(x.long()), tr.tensor(y.long())
            out = net(x)
            loss = loss_fn(out, y)
            total_loss += loss.item()
            loss.backward()
            optimizer.step()
        print i, time() - start, total_loss / len(train_loader), accuracy_on(net, dev_loader, loss_fn)


def main():
    num_classes = 2
    data_set = ExecDataset("trainLabels.csv", "files/train50", "files/benign50", binary=num_classes==2)
    test_indices = data_set.get_test_indices(0.2)
    train_indices = [i for i in range(len(data_set)) if i not in test_indices]
    test_sampler = SubsetRandomSampler(test_indices)
    train_sampler = SubsetRandomSampler(train_indices)
    test_loader = DataLoader(data_set, batch_size=1, sampler=test_sampler, num_workers=1)
    train_loader = DataLoader(data_set, batch_size=1, sampler=train_sampler, num_workers=1)

    net = nn.Sequential(nn.Embedding(257, 8, padding_idx=PAD_IDX),
                        GatedConvolution(),
                        nn.Linear(128, 128),
                        # nn.ReLU(),
                        # nn.Linear(128, 128),
                        # nn.ReLU(),
                        nn.Linear(128, num_classes))
                        # nn.Softmax(dim=1))
    loss_fn = nn.CrossEntropyLoss(size_average=False)
    optimizer = optim.Adam(net.parameters(), lr=0.001)

    train_on(net, optimizer, loss_fn, train_loader, test_loader, epochs=10)


if __name__ == '__main__':
    main()