from sys import argv
large_dataset = "--small" not in argv
if large_dataset:
    malware_data_dir = "../../Malware_data"
    labels_dir = "csv"
    benign_dir = malware_data_dir + "/Benign"

import torch as tr
from torch import nn as nn
from torch.nn import functional as F
from torch import optim as optim
from torch.utils.data import DataLoader
from time import time
from torch.autograd import Variable
if large_dataset:
    from large_dataset_utils import ExecDataset, PAD_IDX
else:
    from small_dataset_utils import ExecDataset, PAD_IDX
from torch.utils.data.sampler import SubsetRandomSampler


class GatedConvolution(nn.Module):
    def __init__(self):
        super(GatedConvolution, self).__init__()
        self.conv = nn.Conv1d(4, 128, 500, stride=500)
        self.convg = nn.Conv1d(4, 128, 500, stride=500)

    def forward(self, x):
        """

        :param x:
        :type x: tr.Tensor
        :return:
        """
        x.transpose_(-1, -2)
        c = self.conv(x.narrow(-2, 0, 4))
        g = F.sigmoid(self.convg(x.narrow(-2, 4, 4)))
        x = F.relu(g * c)
        return F.max_pool1d(x, x.size()[2:]).view(-1, 128)


def accuracy_on(net, dev_loader, loss_fn, device):
    total_loss = good = 0.0
    for x, y in dev_loader:
        if device is not None:
            x, y = x.to(device), y.to(device)
        out = net(x)
        loss = loss_fn(out, y)
        total_loss += loss.item()
        good += (tr.argmax(out, 1) == y).sum().item()
    return total_loss / len(dev_loader.sampler), good / len(dev_loader.sampler) * 100.0


def train_on(net, optimizer, loss_fn, train_loader, dev_loader, epochs=5, device=None):
    for i in xrange(epochs):
        print "#" * 10 + " epoch #{} ".format(i+1) + "#" * 10
        start = time()
        good = total_loss = 0.0
        total = 0
        for j, (x, y) in enumerate(train_loader):
            optimizer.zero_grad()
            if device is not None:
                x, y = x.to(device), y.to(device)
            out = net(x)
            loss = loss_fn(out, y)
            total_loss += loss.item()
            loss.backward()
            optimizer.step()
            good += (tr.argmax(out, 1) == y).sum().item()
            if j % 5 == 4:
                print "gone over {} batches / {} with {}% train accuracy in {}s".format(j+1, len(train_loader),
                                                                                        100.0 * good / (j * train_loader.batch_size),
                                                                                        time() - start)
        dev_loss, dev_acc = accuracy_on(net, dev_loader, loss_fn, device)
        print "=" * 25
        print "epoch, epoch time, train avg. loss, train accuracy, dev avg. loss, dev accuracy"
        print "{:^5}, {:^10.5f}, {:^15f}, {:^14f}%, {:^13f}, {:^12f}%".format(
            i+1, time() - start, total_loss / len(train_loader.sampler), good / len(train_loader.sampler) * 100.0,
            dev_loss, dev_acc)
        print ""


def main():
    batch = 50
    workers = 10
    num_classes = 2 if "--b" in argv else 10
    if num_classes == 2:
        print "BINARYYYYYYYYYYYYYYYYYYYYYY"

    if not large_dataset:
        data_set = ExecDataset("trainLabels.csv", "files/train50", "files/benign50", binary=(num_classes==2))
    else:
        data_set = ExecDataset(labels_dir + "/allLabels.csv", malware_data_dir, benign_dir, binary=(num_classes==2), only_use=[9, 5, 2, 6 ,7])  # todo watch this

    test_indices = data_set.get_test_indices(0.1)
    train_indices = [i for i in range(len(data_set)) if i not in test_indices]
    test_sampler = SubsetRandomSampler(test_indices)
    train_sampler = SubsetRandomSampler(train_indices)

    test_loader = DataLoader(data_set, batch_size=batch, sampler=test_sampler, num_workers=workers)
    train_loader = DataLoader(data_set, batch_size=batch, sampler=train_sampler, num_workers=workers)

    net = nn.Sequential(nn.Embedding(257, 8, padding_idx=PAD_IDX),
                        GatedConvolution(),
                        nn.Linear(128, 128),
                        nn.ReLU(),
                        # nn.Linear(128, 128),
                        # nn.ReLU(),
                        nn.Linear(128, num_classes))
                        # nn.Softmax(dim=1))
    device = None
    if tr.cuda.is_available() and tr.cuda.device_count() > 0:
        device = tr.device("cuda:0")
        net = nn.DataParallel(net)
        net.to(device)

    loss_fn = nn.CrossEntropyLoss(size_average=False)
    optimizer = optim.Adam(net.parameters(), lr=0.01)

    train_on(net, optimizer, loss_fn, train_loader, test_loader, epochs=5, device=device)


if __name__ == '__main__':
    main()